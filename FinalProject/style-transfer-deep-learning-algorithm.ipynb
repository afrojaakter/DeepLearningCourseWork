{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NEURAL STYLE TRANSFER ALGORITHM"},{"metadata":{},"cell_type":"markdown","source":"**Neural style transfer is an optimization technique used to take two images—a content image and a style reference image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.**\n\n**This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network.**"},{"metadata":{},"cell_type":"markdown","source":"![Style transfer image](https://sunshineatnoon.github.io/assets/posts/2017-05-19-a-brief-summary-on-neural-style-transfer/1.png)\n\n**Source - [Xueting Li's Website](https://sunshineatnoon.github.io/posts/2017/05/blog-post-1/)**"},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries and Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv, pd.read_xlsx)\nimport numpy as np # Linear algebra\nimport os\nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img, save_img, img_to_array # data preprocessing\nimport matplotlib.pyplot as plt\nfrom keras.applications import vgg19\nfrom keras.models import Model\nfrom scipy.optimize import fmin_l_bfgs_b\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"StylePath = '../input/best-artworks-of-all-time/images/images/'\nContentPath = '../input/image-classification/validation/validation/travel and adventure/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the path for Base Content Image and the Style image respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_image_path = ContentPath+'5.jpg'\nstyle_image_path = StylePath+'Pablo_Picasso/Pablo_Picasso_10.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimensions of the generated picture.\nwidth, height = load_img(base_image_path).size\nimg_nrows = 400\nimg_ncols = int(width * img_nrows / height)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This function is used to Preprocess the image with help of VGG19.\n**VGGNet** was invented by **VGG (Visual Geometry Group)** from University of Oxford, Though VGGNet was the 1st runner-up, not the winner of the ILSVRC (ImageNet Large Scale Visual Recognition Competition) 2014 in the classification task, which has significantly improvement over ZFNet (The winner in 2013) and AlexNet (The winner in 2012). And GoogLeNet is the winner of ILSVLC 2014, I will also talk about it later.) Nevertheless, VGGNet beats the GoogLeNet and won the localization task in ILSVRC 2014.<br><br>\n**VGG19** is a model, with weights pre-trained on **ImageNet**.**ImageNet**, is a dataset of over 15 millions labeled high-resolution images with around 22,000 categories. ILSVRC uses a subset of ImageNet of around 1000 images in each of 1000 categories. In all, there are roughly 1.3 million training images, 50,000 validation images and 100,000 testing images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dsiplaying Base image\nplt.figure()\nplt.title(\"Base Image\",fontsize=20)\nimg1 = load_img(ContentPath+'10.jpg')\nplt.imshow(img1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying style Image\nplt.figure()\nplt.title(\"Style Image\",fontsize=20)\nimg1 = load_img(StylePath+'Pablo_Picasso/Pablo_Picasso_12.jpg')\nplt.imshow(img1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprcoessing Function\ndef preprocess_image(image_path):\n    from keras.applications import vgg19\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get tensor representations of our images\n\nbase_image = K.variable(preprocess_image(base_image_path))\nstyle_reference_image = K.variable(preprocess_image(style_image_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.image_data_format()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This will contain our generated image\n\nThink of **Variable** in tensorflow as a normal variables which we use in programming languages. We initialize variables, we can modify it later as well. Whereas **placeholder** doesn’t require initial value. Placeholder simply allocates block of memory for future use. Later, we can use feed_dict to feed the data into placeholder. By default, placeholder has an unconstrained shape, which allows you to feed tensors of different shapes in a session."},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will contain our generated image\nif K.image_data_format() == 'channels_first':\n    combination_image = K.placeholder((1,3,img_nrows, img_ncols))\nelse:\n    combination_image = K.placeholder((1,img_nrows, img_ncols,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine the 3 images into a single Keras tensor\ninput_tensor = K.concatenate([base_image,\n                              style_reference_image,\n                              combination_image\n                              ], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the VGG19 model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the VGG19 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nfrom keras.applications.vgg19 import VGG19\nvgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = VGG19(input_tensor=input_tensor,\n              include_top = False,\n              weights=vgg19_weights)\n#model = vgg19.VGG19(input_tensor=input_tensor,\n#                    weights='imagenet', include_top=False)\nprint('Model loaded.')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Athough Vgg19 is basically used for Classification purpose, but here our objective is not to classify rather our objective is to transform a image, so we do not need all the layers of vgg19, we have specially excluded those layers which are used for classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Content layer where will pull our feature maps\ncontent_layers = ['block5_conv2'] \n\n# Style layer we are interested in\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1',\n                'block5_conv1'\n               ]\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\nprint(outputs_dict['block5_conv2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The content Loss\n##### We hence define content loss simply as the L2 distance between the intermediate content representations , taken from higher (later) layers of a pre-trained neural network, for a input image and the target image. As a high level layer produces filters that possess complex raw information for the input image, this is a suitable approximation for judging similarity in terms of content. The equation is shown below:\n![Coontent Loss](https://miro.medium.com/max/788/1*F_v0i8R_Crd3UO-osvcUSw.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\ndef get_content_loss(base_content, target):\n    return K.sum(K.square(target - base_content))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Style Loss\n##### we define the style loss as the L2 distance between the gram matrices of the intermediate style representations for the style image (taken from lower layers of a pre-trained neural network) and the output images. The lower level layers capture more simple image features which best encode the concept of style. Intuitively, Gram matrices we distribute and delocalize spatial information in an image and approximate the “style” of an image. Mathematically, they are matrices are simply multiplication of the image matrix and its transpose.\n![Gram Matrix](https://miro.medium.com/max/788/1*eAcdLzoo_8nnv1g3NTTPzg.png)\n##### The loss function for style is quite similar to out content loss, except that we calculate the Mean Squared Error for the Gram-matrices instead of the raw tensor-outputs from the layers.\n![Style loss](https://miro.medium.com/max/788/1*IrqKU-xT5BaUdOmhd-KU_w.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n# the gram matrix of an image tensor (feature-wise outer product)\ndef gram_matrix(input_tensor):\n    assert K.ndim(input_tensor)==3\n    if K.image_data_format() == 'channels_first':\n        features = K.batch_flatten(input_tensor)\n    else:\n        features = K.batch_flatten(K.permute_dimensions(input_tensor,(2,0,1)))\n    gram = K.dot(features, K.transpose(features))\n    channels = int(input_tensor.shape[-1])\n    a = tf.reshape(input_tensor, [-1, channels])\n    n = tf.shape(a)[0]\n    gram = tf.matmul(a, a, transpose_a=True)\n    return gram#/tf.cast(n, tf.float32)\n\ndef get_style_loss(style, combination):\n    assert K.ndim(style) == 3\n    assert K.ndim(combination) == 3\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_nrows*img_ncols\n    return K.sum(K.square(S - C))#/(4.0 * (channels ** 2) * (size ** 2))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seeing how VGG-19 reduces Input Image after each Convolution Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"content_weight=0.025 \nstyle_weight=1.0\n# combine these loss functions into a single scalar\nloss = K.variable(0.0)\nlayer_features = outputs_dict['block5_conv2']\nbase_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nprint('Layer Feature for Content Layers :: '+str(layer_features))\nprint('Base Image Feature :: '+str(base_image_features))\nprint('Combination Image Feature for Content Layers:: '+str(combination_features)+'\\n')\nloss = loss+ content_weight * get_content_loss(base_image_features,\n                                      combination_features)\n\nfeature_layers = ['block1_conv1', 'block2_conv1',\n                  'block3_conv1', 'block4_conv1',\n                  'block5_conv1']\nfor layer_name in feature_layers:\n    layer_features = outputs_dict[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    print('Layer Feature for Style Layers :: '+str(layer_features))\n    print('Style Image Feature :: '+str(style_reference_features))\n    print('Combination Image Feature for Style Layers:: '+str(combination_features)+'\\n')\n    sl = get_style_loss(style_reference_features, combination_features)\n    loss = loss+ (style_weight / len(feature_layers)) * sl\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features are extracted from each layer in style layers and content layers and their overall loss is calculated from it.\n\n## This deprocess_image function is used return the original format of the Final image after transformation which could be easily read and displayed by Matplotlib."},{"metadata":{"trusted":true},"cell_type":"code","source":"def deprocess_image(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((3, img_nrows, img_ncols))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_nrows, img_ncols, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculation of gradient with respect to loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the gradients of the generated image wrt the loss\ngrads = K.gradients(loss, combination_image)\ngrads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = [loss]\nif isinstance(grads, (list,tuple)):\n    outputs = outputs+ grads\nelse:\n    outputs.append(grads)\nf_outputs = K.function([combination_image], outputs)\nf_outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Athough there are various optimizers but we have used L-BFGS optimizer in this case, I have also gone through research papers where they have used ADAM optimizer to optimize the loss and get the final image.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\nx_opt = preprocess_image(base_image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loss_and_grads(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((1, 3, img_nrows, img_ncols))\n    else:\n        x = x.reshape((1, img_nrows, img_ncols, 3))\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    if len(outs[1:]) == 1:\n        grad_values = outs[1].flatten().astype('float64')\n    else:\n        grad_values = np.array(outs[1:]).flatten().astype('float64')\n    return loss_value, grad_values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The purpose of this **Evaluator** class is to avoid the error **'numpy.ndarray' object is not callable error with optimize.minimize** while running the L-BFGS optimizer for loss minimization.<br><br>\n### You should pass the function itself to minimize, instead of a evaluated value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Evaluator(object):\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = Evaluator()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this segment we run the code upto a given iteration. Although I would not recommend you to use **maxiter** parameter in **fmin_l_bfgs_b** (style transfer algorithm) to set the number of iterations.  Rarther use iteration in for loop to get better results"},{"metadata":{"trusted":true},"cell_type":"code","source":"iterations=40\n# Store our best result\nbest_loss, best_img = float('inf'), None\nfor i in range(iterations):\n    print('Start of iteration', i)\n    x_opt, min_val, info= fmin_l_bfgs_b(evaluator.loss, \n                                        x_opt.flatten(), \n                                        fprime=evaluator.grads,\n                                        maxfun=20,\n                                        disp=True,\n                                       )\n    print('Current loss value:', min_val)\n    if min_val < best_loss:\n        # Update best loss and best image from total loss. \n        best_loss = min_val\n        best_img = x_opt.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save current generated image\nimgx = deprocess_image(best_img.copy())\nplt.imshow(imgx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nplt.title(\"Base Image\",fontsize=20)\nimg_base = load_img(base_image_path)\nplt.imshow(img_base)\n\nplt.subplot(5,5,1+1)\nplt.title(\"Style Image\",fontsize=20)\nimg_style = load_img(style_image_path)\nplt.imshow(img_style)\n\nplt.subplot(5,5,1+2)\nplt.title(\"Final Image\",fontsize=20)\nplt.imshow(imgx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n* In our case the Final images formed are not totally perfect, because the Style image does not totally blend with the Base content image.\n* It could be improved through icreasing the number of iteration, or by trying out a different syle transfer algorithm which could preseve the edges of the base image, or by trying out with different optimizer to minimize gradient and loss."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}